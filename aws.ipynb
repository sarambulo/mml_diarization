{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "from time import time\n",
    "from utils.build_chunks import build_chunks\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from pairs.utils import s3_save_numpy\n",
    "from utils.lip_coordinates import extract_and_crop_lips\n",
    "import dlib\n",
    "import boto3\n",
    "import re\n",
    "import torch\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple\n",
    "from math import floor\n",
    "import re\n",
    "# from pairs.utils import list_s3_files, s3_load_numpy\n",
    "# from pairs.config import S3_BUCKET_NAME\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "# from datasets.LoaderTest import TestDataset\n",
    "\n",
    "s3        = boto3.client(\"s3\")\n",
    "paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "\n",
    "bucket       = \"mmml-proj\"\n",
    "root_prefix  = \"test_preprocessed/\" \n",
    "# Load the test dataset\n",
    "test_dataset = TestDataset(bucket, root_prefix, transform=None,visual_type='lip' )\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "    \n",
    "print(f\"Test dataset size: {len(test_dataset)} samples\")\n",
    "    \n",
    "    # Load the trained model\n",
    "model = load_model(checkpoint_path, embedding_dims=512)\n",
    "    \n",
    "    # Run inference and save predictions\n",
    "# run_inference(model, test_loader, output_csv=\"predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/AnuranjanAnand/Desktop/MML/mml_diarization'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, io, re\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# helpers\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "_s3        = boto3.client(\"s3\")\n",
    "_paginator = _s3.get_paginator(\"list_objects_v2\")\n",
    "\n",
    "def _load_npy(bucket: str, key: str) -> np.ndarray:\n",
    "    \"\"\"Load a .npy object in-memory from S3.\"\"\"\n",
    "    body = _s3.get_object(Bucket=bucket, Key=key)[\"Body\"].read()\n",
    "    return np.load(io.BytesIO(body))\n",
    "\n",
    "def _load_csv(bucket: str, key: str) -> Optional[pd.DataFrame]:\n",
    "    try:\n",
    "        body = _s3.get_object(Bucket=bucket, Key=key)[\"Body\"].read()\n",
    "    except _s3.exceptions.NoSuchKey:\n",
    "        return None\n",
    "    return pd.read_csv(io.BytesIO(body))\n",
    "\n",
    "def _extract_audio_segment(\n",
    "    mel: torch.Tensor,\n",
    "    frame_idx: int,\n",
    "    total_frames: int,\n",
    "    desired_len: int = 22,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Slice or pad a mel spectrogram to length `desired_len` for the given frame.\"\"\"\n",
    "    if mel.ndim == 3:                                # [n_mels, L, T]\n",
    "        seg = mel[:, :, frame_idx]\n",
    "    else:                                            # [n_mels, A]\n",
    "        n_mels, A = mel.shape\n",
    "        a0 = int(frame_idx * A / total_frames)\n",
    "        a1 = max(a0 + 1, int((frame_idx + 1) * A / total_frames))\n",
    "        seg = mel[:, a0:a1]\n",
    "        cur = seg.shape[1]\n",
    "        if cur < desired_len:\n",
    "            seg = torch.nn.functional.pad(seg, (0, desired_len - cur))\n",
    "        elif cur > desired_len:\n",
    "            seg = seg[:, :desired_len]\n",
    "    return seg                                       # [n_mels, desired_len]\n",
    "\n",
    "def _iter_prefixes(bucket: str, root: str):\n",
    "    \"\"\"Yield immediate sub-prefixes (video folders) under `root`.\"\"\"\n",
    "    for page in _paginator.paginate(Bucket=bucket, Prefix=root, Delimiter='/'):\n",
    "        for cp in page.get(\"CommonPrefixes\", []):\n",
    "            yield cp[\"Prefix\"]                       # “…/<video_id>/”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "class TestDataset(Dataset):\n",
    "    \"\"\"\n",
    "    S3-backed test dataset for audio-visual diarization.\n",
    "    Supports `visual_type='face'` or `'lip'`.\n",
    "    Returns (face_tensor, mel_segment, label, metadata).\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        bucket: str,\n",
    "        prefix: str,\n",
    "        transform=None,\n",
    "        visual_type: str = \"face\",\n",
    "        verbose: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert visual_type in {\"face\", \"lip\"}\n",
    "        self.bucket      = bucket\n",
    "        self.prefix      = prefix.rstrip(\"/\") + \"/\"       # ensure trailing /\n",
    "        self.transform   = transform\n",
    "        self.visual_type = visual_type\n",
    "        self.verbose     = verbose\n",
    "\n",
    "        self.samples: List[Dict] = []\n",
    "        vid2speakers     = defaultdict(set)\n",
    "\n",
    "        # ───────── 1. iterate videos ─────────\n",
    "        video_prefixes = list(_iter_prefixes(bucket, self.prefix))\n",
    "        for vid_prefix in tqdm(video_prefixes, desc=\"Scanning videos\"):\n",
    "            video_id   = vid_prefix.split(\"/\")[-2]\n",
    "            csv_key    = f\"{vid_prefix}is_speaking.csv\"\n",
    "            df_labels  = _load_csv(bucket, csv_key)\n",
    "            speak_map  = {}\n",
    "            if df_labels is not None:\n",
    "                for r in df_labels.itertuples():\n",
    "                    speak_map[(int(r.face_id), int(r.frame_id))] = int(r.is_speaking)\n",
    "                    vid2speakers[video_id].add(int(r.face_id))\n",
    "            elif self.verbose:\n",
    "                print(f\"[warn] {csv_key} missing – all labels default to 0\")\n",
    "\n",
    "            # ─────── 2. iterate Chunk_<id>/ folders ───────\n",
    "            for page in _paginator.paginate(\n",
    "                Bucket=bucket, Prefix=vid_prefix, Delimiter='/'\n",
    "            ):\n",
    "                for cp in page.get(\"CommonPrefixes\", []):\n",
    "                    chunk_prefix = cp[\"Prefix\"]                      # …/Chunk_3/\n",
    "                    chunk_id     = chunk_prefix.split(\"/\")[-2].split(\"_\")[-1]\n",
    "\n",
    "                    # list keys in this chunk\n",
    "                    keys = []\n",
    "                    for p in _paginator.paginate(Bucket=bucket, Prefix=chunk_prefix):\n",
    "                        keys.extend(obj[\"Key\"] for obj in p.get(\"Contents\", []))\n",
    "\n",
    "                    mel_key = next((k for k in keys if k.endswith(\"melspectrogram.npy\")), None)\n",
    "                    if mel_key is None:\n",
    "                        if self.verbose:\n",
    "                            print(f\"[warn] no mel in {chunk_prefix}\")\n",
    "                        continue\n",
    "\n",
    "                    face_pattern = f\"{self.visual_type}_(\\\\d+)\\\\.npy$\"\n",
    "                    face_keys = [\n",
    "                        k for k in keys\n",
    "                        if re.search(face_pattern, k)\n",
    "                        and not k.endswith(\"_bboxes.npy\")\n",
    "                    ]\n",
    "                    if not face_keys:\n",
    "                        if self.verbose:\n",
    "                            print(f\"[warn] no {self.visual_type} tracks in {chunk_prefix}\")\n",
    "                        continue\n",
    "\n",
    "                    # ─────── 3. build sample list ───────\n",
    "                    for fk in face_keys:\n",
    "                        speaker_id = int(re.search(face_pattern, fk).group(1))\n",
    "                        face_arr   = _load_npy(bucket, fk)            # [T, C, H, W]\n",
    "                        T          = face_arr.shape[0]\n",
    "\n",
    "                        for frame_idx in range(T):\n",
    "                            label = speak_map.get((speaker_id, frame_idx), 0)\n",
    "                            self.samples.append({\n",
    "                                \"face_key\" : fk,\n",
    "                                \"mel_key\"  : mel_key,\n",
    "                                \"frame_idx\": frame_idx,\n",
    "                                \"label\"    : float(label),\n",
    "                                \"meta\"     : {\n",
    "                                    \"video_id\"    : video_id,\n",
    "                                    \"chunk_id\"    : chunk_id,\n",
    "                                    \"speaker_id\"  : speaker_id,\n",
    "                                    \"frame_idx\"   : frame_idx,\n",
    "                                    \"total_frames\": T,\n",
    "                                    # num_speakers set later\n",
    "                                },\n",
    "                            })\n",
    "\n",
    "        self.vid2nspeakers = {v: len(s) for v, s in vid2speakers.items()}\n",
    "        # fill num_speakers in meta\n",
    "        for s in self.samples:\n",
    "            s[\"meta\"][\"num_speakers\"] = self.vid2nspeakers.get(s[\"meta\"][\"video_id\"], 0)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"[info] built TestDataset with {len(self.samples):,} samples\")\n",
    "\n",
    "    # ─────────────────────────────────────────────────────────\n",
    "    # PyTorch hooks\n",
    "    # ─────────────────────────────────────────────────────────\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        rec   = self.samples[idx]\n",
    "        meta  = rec[\"meta\"]\n",
    "\n",
    "        # face frame\n",
    "        face_arr   = _load_npy(self.bucket, rec[\"face_key\"])\n",
    "        frame      = torch.from_numpy(face_arr[rec[\"frame_idx\"]]).float()\n",
    "        if self.transform:\n",
    "            frame = self.transform(frame)\n",
    "\n",
    "        # mel segment\n",
    "        mel_arr    = _load_npy(self.bucket, rec[\"mel_key\"])\n",
    "        mel_tensor = torch.from_numpy(mel_arr).float()\n",
    "        mel_seg    = _extract_audio_segment(\n",
    "            mel_tensor, rec[\"frame_idx\"], meta[\"total_frames\"], desired_len=22\n",
    "        )\n",
    "\n",
    "        return frame, mel_seg, rec[\"label\"], meta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
