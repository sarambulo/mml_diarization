{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "execution_state": "idle",
   "id": "e0ef9cee-a96b-44b3-9074-258d10c589d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26349/3470941309.py:30: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "/opt/conda/lib/python3.11/site-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import sys\n",
    "sys.path.insert(0, 'datasets')\n",
    "from MSDWild import MSDWildChunks\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import random\n",
    "from torchsummaryX import summary\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from models.VisualOnly import VisualOnlyModel\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from losses.DiarizationLoss import DiarizationLogitsLoss\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Subset\n",
    "from pairs.config import S3_BUCKET_NAME, S3_VIDEO_DIR\n",
    "import os\n",
    "from training.train_multimodal import *\n",
    "from training.visual_train import *\n",
    "\n",
    "CHECKPOINT_PATH = 'model_checkpoints'\n",
    "MULTIMODAL_CHECKPOINT = 'multimodal_checkpoints'\n",
    "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(DEVICE)\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "execution_state": "idle",
   "id": "74304c38-314b-40ce-ad6d-646fca3b91b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory has 523104 pairs\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'few.train.rttm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m partition_path_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfew.train.rttm\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m batch_size\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n\u001b[0;32m----> 6\u001b[0m full_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mMSDWildChunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mS3_VIDEO_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mdata_bucket\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mS3_BUCKET_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpartition_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_path_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                        \u001b[49m\u001b[43msubset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.025\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrefresh_fileset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mml_diarization/datasets/MSDWild.py:92\u001b[0m, in \u001b[0;36mMSDWildChunks.__init__\u001b[0;34m(self, data_path, partition_path, subset, data_bucket, refresh_fileset)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbucket \u001b[38;5;241m=\u001b[39m data_bucket\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubset \u001b[38;5;241m=\u001b[39m subset\n\u001b[0;32m---> 92\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvideo_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_partition_video_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpartition_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpairs_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_pairs_info(video_names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvideo_names)\n\u001b[1;32m     94\u001b[0m pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_pairs), \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpairs_info))\n",
      "File \u001b[0;32m~/mml_diarization/datasets/MSDWild.py:106\u001b[0m, in \u001b[0;36mMSDWildChunks.get_partition_video_ids\u001b[0;34m(self, partition_path)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03mReturns a list of video ID. For example: ['00001', '000002']\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    105\u001b[0m video_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m--> 106\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpartition_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[1;32m    108\u001b[0m         parts \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit()\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'few.train.rttm'"
     ]
    }
   ],
   "source": [
    "data_path = \"preprocessed\"\n",
    "partition_path_train = \"few.train.rttm\"\n",
    "partition_path_val = \"few.train.rttm\"\n",
    "batch_size= 64\n",
    "\n",
    "full_dataset = MSDWildChunks(data_path=S3_VIDEO_DIR,\n",
    "                        data_bucket=S3_BUCKET_NAME,\n",
    "                        partition_path=partition_path_train,\n",
    "                        subset=0.025,\n",
    "                        refresh_fileset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "execution_state": "idle",
   "id": "b6e9e601-c034-4e45-9ed9-abe40a3f6f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_size = len(full_dataset)\n",
    "# indices = list(range(dataset_size))\n",
    "# random.shuffle(indices)\n",
    "# split = int(0.8 * dataset_size)\n",
    "# train_indices = indices[:split]\n",
    "# val_indices   = indices[split:]\n",
    "# train_subset = Subset(full_dataset, train_indices)\n",
    "# val_subset   = Subset(full_dataset, val_indices)\n",
    "# train_loader = DataLoader(\n",
    "#     train_subset,\n",
    "#     batch_size=batch_size,\n",
    "#     shuffle=True,\n",
    "#     collate_fn=full_dataset.build_batch\n",
    "# )\n",
    "# val_loader = DataLoader(\n",
    "#     val_subset,\n",
    "#     batch_size=batch_size,\n",
    "#     shuffle=False,\n",
    "#     collate_fn=full_dataset.build_batch\n",
    "# )\n",
    "# print(f\"Train size: {len(train_subset)}   Val size: {len(val_subset)}\")\n",
    "# model = VisualOnlyModel(embedding_dims=512, num_classes=2)\n",
    "# model = model.float().to(DEVICE)\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "# criterion = DiarizationLogitsLoss(0.3, 0.7)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.8, patience=8)\n",
    "# start_epoch = 0\n",
    "# final_epoch = 100\n",
    "# metrics = {}\n",
    "# best_valid_acc = 0\n",
    "# for epoch in range(start_epoch, final_epoch):\n",
    "#         print(\"\\nEpoch {}/{}\".format(epoch+1, final_epoch))\n",
    "#         curr_lr = float(scheduler.get_last_lr()[0])\n",
    "#         metrics.update({'lr': curr_lr})\n",
    "#         train_acc, train_loss = train_epoch(model, train_loader, optimizer, criterion)\n",
    "#         print(\"\\nEpoch {}/{}: \\nTrain Cls. Acc {:.04f}%\\t Train Cls. Loss {:.04f}\\t Learning Rate {:.04f}\".format(epoch + 1, final_epoch, train_acc, train_loss, curr_lr))\n",
    "#         metrics.update({'train_cls_acc': train_acc, 'train_loss': train_loss})\n",
    "#         valid_acc, valid_loss = evaluate_epoch(model, val_loader, criterion)\n",
    "#         print(\"Val Cls. Acc {:.04f}%\\t Val Cls. Loss {:.04f}\".format(valid_acc, valid_loss))\n",
    "#         metrics.update({'valid_cls_acc': valid_acc, 'valid_loss': valid_loss})\n",
    "#         if epoch%5==4:\n",
    "#             epoch_ckpt_path = Path(CHECKPOINT_PATH, f\"epoch_{epoch+1}.pth\")\n",
    "#             save_model(model, metrics, epoch, epoch_ckpt_path)\n",
    "#         if valid_acc >= best_valid_acc:\n",
    "#             best_valid_acc = valid_acc\n",
    "#             save_model(model, metrics, epoch, Path(CHECKPOINT_PATH, 'best_visual.pth'))\n",
    "#         if scheduler is not None:\n",
    "#             scheduler.step(valid_loss)\n",
    "# save_model(model, metrics, epoch, Path(CHECKPOINT_PATH, 'last_visual.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "execution_state": "idle",
   "id": "627f0cf6-ddc0-4d1c-bc5d-2655de71af61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visual_train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "execution_state": "idle",
   "id": "9f67f4b6-c2ff-4e88-b455-76efdf4ba8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/100\n",
      "training: loss: 0.2320, acc: 0.9627\n",
      "val: loss: 0.1941, acc: 0.9675\n",
      "Saving to multimodal_concat_512_checkpoints/best_model_1.pth\n",
      "\n",
      "Epoch 2/100\n",
      "training: loss: 0.1196, acc: 0.9684\n",
      "val: loss: 0.1182, acc: 0.9675\n",
      "Saving to multimodal_concat_512_checkpoints/best_model_2.pth\n",
      "\n",
      "Epoch 3/100\n",
      "training: loss: 0.1006, acc: 0.9669\n",
      "val: loss: 0.1094, acc: 0.9675\n",
      "Saving to multimodal_concat_512_checkpoints/best_model_3.pth\n",
      "\n",
      "Epoch 4/100\n",
      "training: loss: 0.0929, acc: 0.9660\n",
      "val: loss: 0.1033, acc: 0.9675\n",
      "Saving to multimodal_concat_512_checkpoints/best_model_4.pth\n",
      "\n",
      "Epoch 5/100\n",
      "training: loss: 0.0866, acc: 0.9648\n",
      "val: loss: 0.1046, acc: 0.9675\n",
      "Saving to multimodal_concat_512_checkpoints/epoch_multimodal_5.pth\n",
      "\n",
      "Epoch 6/100\n",
      "Update Schedule to visual_last\n",
      "training: loss: 0.0816, acc: 0.9678\n",
      "val: loss: 0.0829, acc: 0.9675\n",
      "Saving to multimodal_concat_512_checkpoints/best_model_6.pth\n",
      "\n",
      "Epoch 7/100\n",
      "training: loss: 0.0807, acc: 0.9675\n",
      "val: loss: 0.0823, acc: 0.9675\n",
      "Saving to multimodal_concat_512_checkpoints/best_model_7.pth\n",
      "\n",
      "Epoch 8/100\n",
      "training: loss: 0.0800, acc: 0.9677\n",
      "val: loss: 0.0817, acc: 0.9675\n",
      "Saving to multimodal_concat_512_checkpoints/best_model_8.pth\n",
      "\n",
      "Epoch 9/100\n",
      "training: loss: 0.0785, acc: 0.9672\n",
      "val: loss: 0.0811, acc: 0.9675\n",
      "Saving to multimodal_concat_512_checkpoints/best_model_9.pth\n",
      "\n",
      "Epoch 10/100\n",
      "training: loss: 0.0776, acc: 0.9671\n",
      "val: loss: 0.0811, acc: 0.9675\n",
      "Saving to multimodal_concat_512_checkpoints/epoch_multimodal_10.pth\n",
      "Saving to multimodal_concat_512_checkpoints/best_model_10.pth\n",
      "\n",
      "Epoch 11/100\n",
      "Update Schedule to all\n",
      "training: loss: 0.0771, acc: 0.9675\n",
      "val: loss: 0.0804, acc: 0.9675\n",
      "Saving to multimodal_concat_512_checkpoints/best_model_11.pth\n",
      "\n",
      "Epoch 12/100\n",
      "training: loss: 0.0760, acc: 0.9672\n",
      "val: loss: 0.0795, acc: 0.9675\n",
      "Saving to multimodal_concat_512_checkpoints/best_model_12.pth\n",
      "\n",
      "Epoch 13/100\n",
      "training: loss: 0.0749, acc: 0.9677\n",
      "val: loss: 0.0797, acc: 0.9675\n",
      "\n",
      "Epoch 14/100\n",
      "training: loss: 0.0755, acc: 0.9669\n",
      "val: loss: 0.0792, acc: 0.9675\n",
      "Saving to multimodal_concat_512_checkpoints/best_model_14.pth\n",
      "\n",
      "Epoch 15/100\n",
      "training: loss: 0.0752, acc: 0.9669\n",
      "val: loss: 0.0780, acc: 0.9675\n",
      "Saving to multimodal_concat_512_checkpoints/epoch_multimodal_15.pth\n",
      "Saving to multimodal_concat_512_checkpoints/best_model_15.pth\n",
      "\n",
      "Epoch 16/100\n",
      "training: loss: 0.0741, acc: 0.9670\n",
      "val: loss: 0.0772, acc: 0.9675\n",
      "Saving to multimodal_concat_512_checkpoints/best_model_16.pth\n",
      "\n",
      "Epoch 17/100\n",
      "training: loss: 0.0732, acc: 0.9698\n",
      "val: loss: 0.0769, acc: 0.9771\n",
      "Saving to multimodal_concat_512_checkpoints/best_model_17.pth\n",
      "\n",
      "Epoch 18/100\n",
      "training: loss: 0.0725, acc: 0.9689\n",
      "val: loss: 0.0758, acc: 0.9767\n",
      "Saving to multimodal_concat_512_checkpoints/best_model_18.pth\n",
      "\n",
      "Epoch 19/100\n",
      "training: loss: 0.0718, acc: 0.9697\n",
      "val: loss: 0.0744, acc: 0.9778\n",
      "Saving to multimodal_concat_512_checkpoints/best_model_19.pth\n",
      "\n",
      "Epoch 20/100\n",
      "training: loss: 0.0718, acc: 0.9693\n",
      "val: loss: 0.0739, acc: 0.9782\n",
      "Saving to multimodal_concat_512_checkpoints/epoch_multimodal_20.pth\n",
      "Saving to multimodal_concat_512_checkpoints/best_model_20.pth\n",
      "\n",
      "Epoch 21/100\n",
      "training: loss: 0.0705, acc: 0.9722\n",
      "val: loss: 0.0731, acc: 0.9774\n",
      "Saving to multimodal_concat_512_checkpoints/best_model_21.pth\n",
      "\n",
      "Epoch 22/100\n",
      "training: loss: 0.0710, acc: 0.9711\n",
      "val: loss: 0.0723, acc: 0.9782\n",
      "Saving to multimodal_concat_512_checkpoints/best_model_22.pth\n",
      "\n",
      "Epoch 23/100\n",
      "training: loss: 0.0697, acc: 0.9733\n",
      "val: loss: 0.0727, acc: 0.9774\n",
      "\n",
      "Epoch 24/100\n",
      "training: loss: 0.0703, acc: 0.9729\n",
      "val: loss: 0.0723, acc: 0.9767\n",
      "Saving to multimodal_concat_512_checkpoints/best_model_24.pth\n",
      "\n",
      "Epoch 25/100\n",
      "training: loss: 0.0691, acc: 0.9717\n",
      "val: loss: 0.0711, acc: 0.9782\n",
      "Saving to multimodal_concat_512_checkpoints/epoch_multimodal_25.pth\n",
      "Saving to multimodal_concat_512_checkpoints/best_model_25.pth\n",
      "\n",
      "Epoch 26/100\n",
      "training: loss: 0.0687, acc: 0.9729\n",
      "val: loss: 0.0713, acc: 0.9782\n",
      "\n",
      "Epoch 27/100\n",
      "training: loss: 0.0684, acc: 0.9736\n",
      "val: loss: 0.0715, acc: 0.9778\n",
      "\n",
      "Epoch 28/100\n",
      "training: loss: 0.0684, acc: 0.9732\n",
      "val: loss: 0.0706, acc: 0.9782\n",
      "Saving to multimodal_concat_512_checkpoints/best_model_28.pth\n",
      "\n",
      "Epoch 29/100\n",
      "training: loss: 0.0671, acc: 0.9746\n",
      "val: loss: 0.0702, acc: 0.9794\n",
      "Saving to multimodal_concat_512_checkpoints/best_model_29.pth\n",
      "\n",
      "Epoch 30/100\n",
      "training: loss: 0.0675, acc: 0.9737\n",
      "val: loss: 0.0701, acc: 0.9790\n",
      "Saving to multimodal_concat_512_checkpoints/epoch_multimodal_30.pth\n",
      "Saving to multimodal_concat_512_checkpoints/best_model_30.pth\n",
      "\n",
      "Epoch 31/100\n",
      "training: loss: 0.0669, acc: 0.9748\n",
      "val: loss: 0.0717, acc: 0.9778\n",
      "\n",
      "Epoch 32/100\n",
      "training: loss: 0.0660, acc: 0.9734\n",
      "val: loss: 0.0702, acc: 0.9771\n",
      "\n",
      "Epoch 33/100\n",
      "training: loss: 0.0675, acc: 0.9742\n",
      "val: loss: 0.0713, acc: 0.9759\n",
      "\n",
      "Epoch 34/100\n",
      "training: loss: 0.0665, acc: 0.9735\n",
      "val: loss: 0.0700, acc: 0.9774\n",
      "Saving to multimodal_concat_512_checkpoints/best_model_34.pth\n",
      "\n",
      "Epoch 35/100\n",
      "training: loss: 0.0653, acc: 0.9758\n",
      "val: loss: 0.0690, acc: 0.9786\n",
      "Saving to multimodal_concat_512_checkpoints/epoch_multimodal_35.pth\n",
      "Saving to multimodal_concat_512_checkpoints/best_model_35.pth\n",
      "\n",
      "Epoch 36/100\n",
      "training: loss: 0.0653, acc: 0.9750\n",
      "val: loss: 0.0691, acc: 0.9786\n",
      "\n",
      "Epoch 37/100\n",
      "training: loss: 0.0647, acc: 0.9759\n",
      "val: loss: 0.0687, acc: 0.9778\n",
      "Saving to multimodal_concat_512_checkpoints/best_model_37.pth\n",
      "\n",
      "Epoch 38/100\n",
      "training: loss: 0.0649, acc: 0.9746\n",
      "val: loss: 0.0698, acc: 0.9782\n",
      "\n",
      "Epoch 39/100\n",
      "training: loss: 0.0642, acc: 0.9755\n",
      "val: loss: 0.0705, acc: 0.9797\n",
      "\n",
      "Epoch 40/100\n",
      "training: loss: 0.0640, acc: 0.9769\n",
      "val: loss: 0.0705, acc: 0.9725\n",
      "Saving to multimodal_concat_512_checkpoints/epoch_multimodal_40.pth\n",
      "\n",
      "Epoch 41/100\n",
      "training: loss: 0.0635, acc: 0.9750\n",
      "val: loss: 0.0689, acc: 0.9786\n",
      "\n",
      "Epoch 42/100\n",
      "training: loss: 0.0632, acc: 0.9769\n",
      "val: loss: 0.0674, acc: 0.9794\n",
      "Saving to multimodal_concat_512_checkpoints/best_model_42.pth\n",
      "\n",
      "Epoch 43/100\n",
      "training: loss: 0.0618, acc: 0.9775\n",
      "val: loss: 0.0668, acc: 0.9794\n",
      "Saving to multimodal_concat_512_checkpoints/best_model_43.pth\n",
      "\n",
      "Epoch 44/100\n",
      "training: loss: 0.0616, acc: 0.9772\n",
      "val: loss: 0.0679, acc: 0.9797\n",
      "\n",
      "Epoch 45/100\n",
      "training: loss: 0.0627, acc: 0.9747\n",
      "val: loss: 0.0683, acc: 0.9778\n",
      "Saving to multimodal_concat_512_checkpoints/epoch_multimodal_45.pth\n",
      "\n",
      "Epoch 46/100\n",
      "training: loss: 0.0614, acc: 0.9773\n",
      "val: loss: 0.0677, acc: 0.9774\n",
      "\n",
      "Epoch 47/100\n",
      "training: loss: 0.0620, acc: 0.9771\n",
      "val: loss: 0.0681, acc: 0.9778\n",
      "\n",
      "Epoch 48/100\n",
      "training: loss: 0.0614, acc: 0.9772\n",
      "val: loss: 0.0725, acc: 0.9717\n",
      "\n",
      "Epoch 49/100\n",
      "training: loss: 0.0616, acc: 0.9771\n",
      "val: loss: 0.0663, acc: 0.9790\n",
      "Saving to multimodal_concat_512_checkpoints/best_model_49.pth\n",
      "\n",
      "Epoch 50/100\n",
      "training: loss: 0.0608, acc: 0.9773\n",
      "val: loss: 0.0689, acc: 0.9744\n",
      "Saving to multimodal_concat_512_checkpoints/epoch_multimodal_50.pth\n",
      "\n",
      "Epoch 51/100\n",
      "training: loss: 0.0605, acc: 0.9775\n",
      "val: loss: 0.0700, acc: 0.9725\n",
      "\n",
      "Epoch 52/100\n",
      "training: loss: 0.0603, acc: 0.9781\n",
      "val: loss: 0.0666, acc: 0.9790\n",
      "\n",
      "Epoch 53/100\n",
      "training: loss: 0.0604, acc: 0.9774\n",
      "val: loss: 0.0694, acc: 0.9721\n",
      "\n",
      "Epoch 54/100\n",
      "training: loss: 0.0599, acc: 0.9781\n",
      "val: loss: 0.0674, acc: 0.9782\n",
      "\n",
      "Epoch 55/100\n",
      "training: loss: 0.0594, acc: 0.9777\n",
      "val: loss: 0.0690, acc: 0.9736\n",
      "Saving to multimodal_concat_512_checkpoints/epoch_multimodal_55.pth\n",
      "\n",
      "Epoch 56/100\n",
      "training: loss: 0.0587, acc: 0.9777\n",
      "val: loss: 0.0674, acc: 0.9767\n",
      "\n",
      "Epoch 57/100\n",
      "training: loss: 0.0589, acc: 0.9796\n",
      "val: loss: 0.0687, acc: 0.9729\n",
      "\n",
      "Epoch 58/100\n",
      "training: loss: 0.0578, acc: 0.9793\n",
      "val: loss: 0.0670, acc: 0.9774\n",
      "\n",
      "Epoch 59/100\n",
      "training: loss: 0.0578, acc: 0.9787\n",
      "val: loss: 0.0652, acc: 0.9801\n",
      "Saving to multimodal_concat_512_checkpoints/best_model_59.pth\n",
      "\n",
      "Epoch 60/100\n",
      "training: loss: 0.0574, acc: 0.9807\n",
      "val: loss: 0.0660, acc: 0.9786\n",
      "Saving to multimodal_concat_512_checkpoints/epoch_multimodal_60.pth\n",
      "\n",
      "Epoch 61/100\n",
      "training: loss: 0.0573, acc: 0.9793\n",
      "val: loss: 0.0668, acc: 0.9771\n",
      "\n",
      "Epoch 62/100\n",
      "training: loss: 0.0570, acc: 0.9804\n",
      "val: loss: 0.0693, acc: 0.9778\n",
      "\n",
      "Epoch 63/100\n",
      "training: loss: 0.0569, acc: 0.9789\n",
      "val: loss: 0.0675, acc: 0.9767\n",
      "\n",
      "Epoch 64/100\n",
      "training: loss: 0.0563, acc: 0.9815\n",
      "val: loss: 0.0647, acc: 0.9774\n",
      "Saving to multimodal_concat_512_checkpoints/best_model_64.pth\n",
      "\n",
      "Epoch 65/100\n",
      "training: loss: 0.0554, acc: 0.9817\n",
      "val: loss: 0.0648, acc: 0.9782\n",
      "Saving to multimodal_concat_512_checkpoints/epoch_multimodal_65.pth\n",
      "\n",
      "Epoch 66/100\n",
      "training: loss: 0.0560, acc: 0.9811\n",
      "val: loss: 0.0688, acc: 0.9744\n",
      "\n",
      "Epoch 67/100\n",
      "training: loss: 0.0550, acc: 0.9823\n",
      "val: loss: 0.0653, acc: 0.9778\n",
      "\n",
      "Epoch 68/100\n",
      "training: loss: 0.0550, acc: 0.9816\n",
      "val: loss: 0.0659, acc: 0.9767\n",
      "\n",
      "Epoch 69/100\n",
      "training: loss: 0.0544, acc: 0.9809\n",
      "val: loss: 0.0656, acc: 0.9782\n",
      "\n",
      "Epoch 70/100\n",
      "training: loss: 0.0537, acc: 0.9824\n",
      "val: loss: 0.0661, acc: 0.9748\n",
      "Saving to multimodal_concat_512_checkpoints/epoch_multimodal_70.pth\n",
      "\n",
      "Epoch 71/100\n",
      "training: loss: 0.0536, acc: 0.9816\n",
      "val: loss: 0.0674, acc: 0.9759\n",
      "\n",
      "Epoch 72/100\n",
      "training: loss: 0.0532, acc: 0.9820\n",
      "val: loss: 0.0657, acc: 0.9755\n",
      "\n",
      "Epoch 73/100\n",
      "training: loss: 0.0524, acc: 0.9841\n",
      "val: loss: 0.0728, acc: 0.9717\n",
      "\n",
      "Epoch 74/100\n",
      "training: loss: 0.0539, acc: 0.9826\n",
      "val: loss: 0.0654, acc: 0.9763\n",
      "\n",
      "Epoch 75/100\n",
      "training: loss: 0.0529, acc: 0.9828\n",
      "val: loss: 0.0681, acc: 0.9721\n",
      "Saving to multimodal_concat_512_checkpoints/epoch_multimodal_75.pth\n",
      "\n",
      "Epoch 76/100\n",
      "training: loss: 0.0523, acc: 0.9823\n",
      "val: loss: 0.0670, acc: 0.9771\n",
      "\n",
      "Epoch 77/100\n",
      "training: loss: 0.0516, acc: 0.9840\n",
      "val: loss: 0.0663, acc: 0.9744\n",
      "\n",
      "Epoch 78/100\n",
      "training: loss: 0.0511, acc: 0.9840\n",
      "val: loss: 0.0692, acc: 0.9748\n",
      "\n",
      "Epoch 79/100\n",
      "training: loss: 0.0527, acc: 0.9833\n",
      "val: loss: 0.0673, acc: 0.9748\n",
      "\n",
      "Epoch 80/100\n",
      "training: loss: 0.0510, acc: 0.9834\n",
      "val: loss: 0.0673, acc: 0.9748\n",
      "Saving to multimodal_concat_512_checkpoints/epoch_multimodal_80.pth\n",
      "\n",
      "Epoch 81/100\n",
      "training: loss: 0.0507, acc: 0.9843\n",
      "val: loss: 0.0673, acc: 0.9763\n",
      "\n",
      "Epoch 82/100\n",
      "training: loss: 0.0498, acc: 0.9850\n",
      "val: loss: 0.0720, acc: 0.9729\n",
      "\n",
      "Epoch 83/100\n",
      "training: loss: 0.0486, acc: 0.9860\n",
      "val: loss: 0.0698, acc: 0.9752\n",
      "\n",
      "Epoch 84/100\n",
      "training: loss: 0.0490, acc: 0.9845\n",
      "val: loss: 0.0715, acc: 0.9725\n",
      "\n",
      "Epoch 85/100\n",
      "training: loss: 0.0491, acc: 0.9852\n",
      "val: loss: 0.0711, acc: 0.9759\n",
      "Saving to multimodal_concat_512_checkpoints/epoch_multimodal_85.pth\n",
      "\n",
      "Epoch 86/100\n",
      "training: loss: 0.0486, acc: 0.9846\n",
      "val: loss: 0.0711, acc: 0.9732\n",
      "\n",
      "Epoch 87/100\n",
      "training: loss: 0.0479, acc: 0.9859\n",
      "val: loss: 0.0713, acc: 0.9748\n",
      "\n",
      "Epoch 88/100\n",
      "training: loss: 0.0477, acc: 0.9880\n",
      "val: loss: 0.0787, acc: 0.9786\n",
      "\n",
      "Epoch 89/100\n",
      "training: loss: 0.0481, acc: 0.9863\n",
      "val: loss: 0.0709, acc: 0.9752\n",
      "\n",
      "Epoch 90/100\n",
      "training: loss: 0.0475, acc: 0.9864\n",
      "val: loss: 0.0703, acc: 0.9744\n",
      "Saving to multimodal_concat_512_checkpoints/epoch_multimodal_90.pth\n",
      "\n",
      "Epoch 91/100\n",
      "training: loss: 0.0463, acc: 0.9873\n",
      "val: loss: 0.0756, acc: 0.9748\n",
      "\n",
      "Epoch 92/100\n",
      "training: loss: 0.0478, acc: 0.9856\n",
      "val: loss: 0.0790, acc: 0.9675\n",
      "\n",
      "Epoch 93/100\n",
      "training: loss: 0.0468, acc: 0.9867\n",
      "val: loss: 0.0724, acc: 0.9729\n",
      "\n",
      "Epoch 94/100\n",
      "training: loss: 0.0448, acc: 0.9879\n",
      "val: loss: 0.0736, acc: 0.9736\n",
      "\n",
      "Epoch 95/100\n",
      "training: loss: 0.0450, acc: 0.9889\n",
      "val: loss: 0.0754, acc: 0.9729\n",
      "Saving to multimodal_concat_512_checkpoints/epoch_multimodal_95.pth\n",
      "\n",
      "Epoch 96/100\n",
      "training: loss: 0.0455, acc: 0.9875\n",
      "val: loss: 0.0748, acc: 0.9729\n",
      "\n",
      "Epoch 97/100\n",
      "training: loss: 0.0458, acc: 0.9875\n",
      "val: loss: 0.0780, acc: 0.9717\n",
      "\n",
      "Epoch 98/100\n",
      "training: loss: 0.0426, acc: 0.9898\n",
      "val: loss: 0.0757, acc: 0.9729\n",
      "\n",
      "Epoch 99/100\n",
      "training: loss: 0.0434, acc: 0.9896\n",
      "val: loss: 0.0776, acc: 0.9721\n",
      "\n",
      "Epoch 100/100\n",
      "training: loss: 0.0441, acc: 0.9890\n",
      "val: loss: 0.0736, acc: 0.9706\n",
      "Saving to multimodal_concat_512_checkpoints/epoch_multimodal_100.pth\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "##LOAD CHECKPOINTS\n",
    "audio_model = CompactAudioEmbedding(input_dim=40, embedding_dim=512, dropout_rate=0.3)\n",
    "old_audio_dict = torch.load(\"12.pth\", map_location=DEVICE)\n",
    "\n",
    "new_audio_state_dict = {}\n",
    "for key, value in old_audio_dict.items():\n",
    "    if key.startswith('classifier'):\n",
    "        continue\n",
    "    new_key = key.replace('encoder.', '')\n",
    "    new_audio_state_dict[new_key] = value\n",
    "    \n",
    "audio_model.load_state_dict(new_audio_state_dict)\n",
    "\n",
    "visual_model = ResNet34(embedding_dims = 512)\n",
    "\n",
    "vid_state_dict = torch.load(\"model_checkpoints/epoch_55.pth\", map_location=DEVICE)['model_state_dict']\n",
    "\n",
    "new_vid_state_dict = {}\n",
    "for key, value in vid_state_dict.items():\n",
    "    if key.startswith('visual_encoder.'):\n",
    "        new_key = key.replace('visual_encoder.', '')\n",
    "        new_vid_state_dict[new_key] = value\n",
    "    elif key.startswith('classifier'):\n",
    "        continue\n",
    "    else:\n",
    "        new_vid_state_dict[key] = value\n",
    "visual_model.load_state_dict(new_vid_state_dict)\n",
    "\n",
    "fusion_model = ConcatenationFusionModel(\n",
    "    audio_model=audio_model,\n",
    "    visual_model=visual_model,\n",
    "    fusion_dim=512,\n",
    "    embedding_dim=512,\n",
    "    fusion_type=\"additive\",\n",
    ").to(DEVICE)\n",
    "\n",
    "# train_rttm_path = \"data_sample/all.rttm\"\n",
    "# train_data_path = \"preprocessed\"\n",
    "\n",
    "# train_dataset_full = MSDWildChunks(\n",
    "#     data_path=train_data_path, rttm_path=train_rttm_path, subset=0.8\n",
    "# )\n",
    "\n",
    "# # split few_train into train + val\n",
    "train_size = int(0.8 * full_dataset.length)\n",
    "val_size = full_dataset.length - train_size\n",
    "\n",
    "train_subset, val_subset = random_split(\n",
    "    full_dataset,\n",
    "    [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(69),\n",
    ")\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(\n",
    "    train_subset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_subset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "unfreeze_schedule = {\n",
    "    5: \"audio_last\",  # after 2 epochs - unfreeze last layers of audio encoder\n",
    "    5: \"visual_last\",  # same as above for visual\n",
    "    10: \"all\",  # unfreeze everything\n",
    "}\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "    [\n",
    "        {\"params\": fusion_model.fusion_linear.parameters()},\n",
    "        {\"params\": fusion_model.bn.parameters()},\n",
    "        {\"params\": fusion_model.fusion_embedding.parameters()},\n",
    "        {\"params\": fusion_model.classifier.parameters()},\n",
    "    ],\n",
    "    lr=0.001,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "criterion = DiarizationLoss(triplet_lambda=0.3, bce_lambda=0.7)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2, factor=0.8)\n",
    "\n",
    "trained_model, best_val_loss = train_fusion_model(\n",
    "    fusion_model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    scheduler,\n",
    "    DEVICE,\n",
    "    num_epochs=100,\n",
    "    unfreeze_schedule=unfreeze_schedule,\n",
    "    checkpoint_dir = \"multimodal_concat_512_checkpoints\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a474f0f-ba17-4fcf-a111-797d6151d1c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4791c6-c3d5-4549-867b-f27fbb5341c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
