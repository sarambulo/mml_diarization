{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "390f48a6-eaeb-403f-9e92-9fc46131b824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from datasets.MSDWild import MSDWildChunks\n",
    "from datasets.MSDWildOptimized import UpfrontNPZDataset, LazyNPZDataset\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5ae64b6-1021-4537-80c1-0a8b2e1422b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_BUCKET_NAME = 'mmml-proj'\n",
    "S3_VIDEO_DIR = 'preprocessed'\n",
    "TRAIN_DATA_PATH = os.path.join(\"s3://\", S3_BUCKET_NAME, S3_VIDEO_DIR)\n",
    "TRAIN_RTTM_PATH = \"few.train.rttm\"\n",
    "SUBSET = 0.025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb3886d-bb1a-451a-9481-6e861a1720de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory has 523104 pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Pair Metadata for Videos: 100%|██████████| 2476/2476 [02:44<00:00, 15.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded metadata for 1379378 pairs\n",
      "Reducing number of pairs from 523104 to 13077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Triplet Files:  30%|██▉       | 3888/13077 [08:39<31:54:43, 12.50s/it]"
     ]
    }
   ],
   "source": [
    "# Est Time: 27.75 minutes\n",
    "start = time.time()\n",
    "dataset = MSDWildChunks(data_path=S3_VIDEO_DIR,\n",
    "                        data_bucket=S3_BUCKET_NAME,\n",
    "                        partition_path=TRAIN_RTTM_PATH,\n",
    "                        subset=SUBSET,\n",
    "                        refresh_fileset=False)\n",
    "end = time.time()\n",
    "execution_time_minutes = (end - start) / 60\n",
    "\n",
    "print(f\"Execution time: {execution_time_minutes:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a064efb1-1ab0-4183-88dc-a5db7262e52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0.00 minutes\n"
     ]
    }
   ],
   "source": [
    "# Measured Time: 0.73 Minutes\n",
    "start = time.time()\n",
    "dataset = LazyNPZDataset(npz_dir=\"test_batched_triplets\",\n",
    "                            num_batches=14,\n",
    "                            batch_size=1000,\n",
    "                            bucket=S3_BUCKET_NAME)\n",
    "end = time.time()\n",
    "execution_time_minutes = (end - start) / 60\n",
    "\n",
    "print(f\"Execution time: {execution_time_minutes:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "487939d3-de56-41df-8b43-0d04e5d5befb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def collate_fn(batch):\n",
    "    # Extract each feature: do the zip thing\n",
    "    video_data, audio_data, is_speaking = list(zip(*batch))\n",
    "    # Padding: NOTE: Not necessary\n",
    "    # Stack:\n",
    "    video_data = torch.stack(video_data)\n",
    "    audio_data = torch.stack(audio_data)\n",
    "    is_speaking = torch.tensor(is_speaking)\n",
    "    # Return tuple((N, video_data, melspectrogram), (N, video_data, melspectrogram), (N, video_data, melspectrogram))\n",
    "    # (N, C, H, W), (N, Bands, T) x3 (ask Prachi)\n",
    "    batch_data = {\n",
    "        \"video_data\": video_data,\n",
    "        \"audio_data\": audio_data,\n",
    "        \"labels\": is_speaking,\n",
    "    }\n",
    "    return batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a1d89ad-d587-4699-ab65-e79349588d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=2,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=2,\n",
    "        pin_memory=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2ceedd15-380f-406a-b4bb-aa4e79c33ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_visual_triplet(images):\n",
    "    images_rgb = np.transpose(\n",
    "        images, axes=(0, 2, 3, 1)\n",
    "    )  # Transpose to (num_images, height, width, channels)\n",
    "\n",
    "    # Plot the images side by side\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    fig.suptitle(\"Three RGB Images Side by Side\")\n",
    "\n",
    "    names = [\"Anchor\", \"Positive\", \"Negative\"]\n",
    "\n",
    "    for i in range(3):\n",
    "        axs[i].imshow(images_rgb[i])  # Display each image\n",
    "        axs[i].set_title(names[i])\n",
    "        axs[i].axis(\"off\")  # Turn off axis labels for cleaner display\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c01a42f3-e84d-4411-aa7f-18f1976d29fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['video_data', 'audio_data', 'labels'])\n",
      "torch.Size([2, 3, 3, 112, 112])\n",
      "torch.Size([2, 3, 30, 22])\n",
      "tensor([1, 1])\n",
      "dict_keys(['video_data', 'audio_data', 'labels'])\n",
      "torch.Size([2, 3, 3, 112, 112])\n",
      "torch.Size([2, 3, 30, 22])\n",
      "tensor([1, 1])\n",
      "dict_keys(['video_data', 'audio_data', 'labels'])\n",
      "torch.Size([2, 3, 3, 112, 112])\n",
      "torch.Size([2, 3, 30, 22])\n",
      "tensor([1, 1])\n",
      "dict_keys(['video_data', 'audio_data', 'labels'])\n",
      "torch.Size([2, 3, 3, 112, 112])\n",
      "torch.Size([2, 3, 30, 22])\n",
      "tensor([1, 1])\n",
      "dict_keys(['video_data', 'audio_data', 'labels'])\n",
      "torch.Size([2, 3, 3, 112, 112])\n",
      "torch.Size([2, 3, 30, 22])\n",
      "tensor([1, 1])\n",
      "dict_keys(['video_data', 'audio_data', 'labels'])\n",
      "torch.Size([2, 3, 3, 112, 112])\n",
      "torch.Size([2, 3, 30, 22])\n",
      "tensor([1, 1])\n",
      "dict_keys(['video_data', 'audio_data', 'labels'])\n",
      "torch.Size([2, 3, 3, 112, 112])\n",
      "torch.Size([2, 3, 30, 22])\n",
      "tensor([1, 1])\n",
      "dict_keys(['video_data', 'audio_data', 'labels'])\n",
      "torch.Size([2, 3, 3, 112, 112])\n",
      "torch.Size([2, 3, 30, 22])\n",
      "tensor([1, 1])\n",
      "dict_keys(['video_data', 'audio_data', 'labels'])\n",
      "torch.Size([2, 3, 3, 112, 112])\n",
      "torch.Size([2, 3, 30, 22])\n",
      "tensor([1, 1])\n",
      "dict_keys(['video_data', 'audio_data', 'labels'])\n",
      "torch.Size([2, 3, 3, 112, 112])\n",
      "torch.Size([2, 3, 30, 22])\n",
      "tensor([1, 1])\n",
      "dict_keys(['video_data', 'audio_data', 'labels'])\n",
      "torch.Size([2, 3, 3, 112, 112])\n",
      "torch.Size([2, 3, 30, 22])\n",
      "tensor([1, 1])\n",
      "dict_keys(['video_data', 'audio_data', 'labels'])\n",
      "torch.Size([2, 3, 3, 112, 112])\n",
      "torch.Size([2, 3, 30, 22])\n",
      "tensor([1, 1])\n",
      "dict_keys(['video_data', 'audio_data', 'labels'])\n",
      "torch.Size([2, 3, 3, 112, 112])\n",
      "torch.Size([2, 3, 30, 22])\n",
      "tensor([1, 1])\n",
      "dict_keys(['video_data', 'audio_data', 'labels'])\n",
      "torch.Size([2, 3, 3, 112, 112])\n",
      "torch.Size([2, 3, 30, 22])\n",
      "tensor([1, 1])\n",
      "dict_keys(['video_data', 'audio_data', 'labels'])\n",
      "torch.Size([2, 3, 3, 112, 112])\n",
      "torch.Size([2, 3, 30, 22])\n",
      "tensor([1, 1])\n"
     ]
    }
   ],
   "source": [
    "numiters = 15\n",
    "for i in loader:\n",
    "    print(i.keys())\n",
    "    video = i[\"video_data\"]\n",
    "    audio = i[\"audio_data\"]\n",
    "    label = i[\"labels\"]\n",
    "    print(video.shape)\n",
    "    print(audio.shape)\n",
    "    print(label)\n",
    "    # visualize_visual_triplet(video[0])\n",
    "    # visualize_visual_triplet(video[1])\n",
    "    numiters-=1\n",
    "    if numiters < 1: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1cbcc8-88cd-4dff-b83f-47d7559db45d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
